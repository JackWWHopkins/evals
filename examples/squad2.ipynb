{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an SQUAD2 Eval\n",
    "\n",
    "This notebook shows how to:\n",
    "- Build and run an eval\n",
    "- Load the results and into a Pandas Dataframe\n",
    "\n",
    "We use the `evals.elsuite.basic.match:Match` Eval class here to check whether new completions match the correct answer. Under the hood, it will generate a completion with the choice of model for each prompt, check if the completion matches the true answer, then logs a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "\n",
    "data_pth = \"squad2_extract\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/jackhopkins/PycharmProjects/evals/examples\r\n",
      "\u001B[31mERROR: file:///Users/jackhopkins/PycharmProjects/evals/examples does not appear to be a Python project: neither 'setup.py' nor 'pyproject.toml' found.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mNote: you may need to restart the kernel to use updated packages.\n",
      "Downloading squad_2_0_train.json to squad2_shuffled...\n",
      "squad_2_0_train.json downloaded successfully to squad2_shuffled.\n",
      "Downloading squad_2_0_dev.json to squad2_shuffled...\n",
      "squad_2_0_dev.json downloaded successfully to squad2_shuffled.\n"
     ]
    }
   ],
   "source": [
    "# Install, and download SQUAD if you haven't already\n",
    "import json\n",
    "%pip install -e .\n",
    "\n",
    "!pip install -q wget\n",
    "\n",
    "import wget\n",
    "\n",
    "def download_squad(version='2.0', target_dir='.'):\n",
    "    if version not in ['1.1', '2.0']:\n",
    "        raise ValueError(\"SQuAD version must be '1.1' or '2.0'\")\n",
    "\n",
    "    squad_url = {\n",
    "        '1.1': {\n",
    "            'train': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v1.1.json',\n",
    "            'dev': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v1.1.json'\n",
    "        },\n",
    "        '2.0': {\n",
    "            'train': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/train-v2.0.json',\n",
    "            'dev': 'https://rajpurkar.github.io/SQuAD-explorer/dataset/dev-v2.0.json',\n",
    "        }\n",
    "    }\n",
    "\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    for split, url in squad_url[version].items():\n",
    "        filename = f'squad_{version.replace(\".\", \"_\")}_{split}.json'\n",
    "        file_path = os.path.join(target_dir, filename)\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f'Downloading {filename} to {target_dir}...')\n",
    "            wget.download(url, file_path)\n",
    "            print(f'{filename} downloaded successfully to {target_dir}.')\n",
    "        else:\n",
    "            print(f'{filename} already exists in {target_dir}.')\n",
    "\n",
    "download_squad('2.0', target_dir=data_pth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Assuming this notebook is in examples/\n",
    "registry_pth = os.path.join(os.getcwd(), \"../evals/registry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the prompts using Chat format. We support converting Chat conversations to text for non-Chat models\n",
    "\n",
    "sys_msg = \"The following are questions about {}. Extract the exact answer if it exists, else write 'None'.\"\n",
    "def create_chat_prompt(sys_msg, question, paragraph, subject):\n",
    "    user_prompt = f\"Paragraph:{paragraph}\\nQuestion:{question}\\nAnswer:\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": sys_msg.format(subject)}, \n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "def create_chat_example(question, paragraph, correct_answer):\n",
    "    \"\"\"\n",
    "    Form few-shot prompts in the recommended format: https://github.com/openai/openai-python/blob/main/chatml.md#few-shot-prompting\n",
    "    \"\"\"\n",
    "    user_prompt = f\"Paragraph:{paragraph}\\nQuestion:{question}\\nAnswer:\"\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": user_prompt, \"name\": \"example_user\"},\n",
    "        {\"role\": \"system\", \"content\": correct_answer, \"name\": \"example_assistant\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squad_2_0_dev.json\n",
      "squad_2_0_train.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def squad_json_to_dataframe(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        squad_data = json.load(file)\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for article in squad_data['data']:\n",
    "        title = article['title']\n",
    "        for paragraph in article['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qas in paragraph['qas']:\n",
    "                question = qas['question']\n",
    "                if qas['is_impossible']:\n",
    "                    answer = \"None\"\n",
    "                else:\n",
    "                    answer = qas['answers'][0]['text']\n",
    "                data.append({'Question': question, 'Paragraph': context, 'Answer': answer, 'Subject': title})\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['Question', 'Paragraph', 'Answer', 'Subject'])\n",
    "    return df\n",
    "\n",
    "subjects = sorted([f for f in os.listdir(data_pth)])\n",
    "\n",
    "registry_yaml = {}\n",
    "samples_pth, few_shot_pth = '', ''\n",
    "for subject in subjects:\n",
    "\n",
    "    subject_pth = os.path.join(registry_pth, \"data\", data_pth)\n",
    "\n",
    "    os.makedirs(subject_pth, exist_ok=True)\n",
    "    dataset = squad_json_to_dataframe(os.path.join(data_pth, subject))\n",
    "    print(subject)\n",
    "    if subject.endswith('_dev.json'):\n",
    "         # Create few-shot prompts\n",
    "        dataset[\"sample\"] = dataset.apply(lambda x: create_chat_example(x[\"Question\"], x[\"Paragraph\"], x['Answer']), axis=1)\n",
    "        few_shot_pth = os.path.join(subject_pth, \"few_shot.jsonl\")\n",
    "        dataset[[\"sample\"]].sample(frac=1).head(400).to_json(few_shot_pth, lines=True, orient=\"records\")\n",
    "        eval_id = f\"match_{data_pth}_dev\"\n",
    "\n",
    "    else:\n",
    "        # Create test prompts and ideal completions\n",
    "        dataset[\"input\"] = dataset.apply(lambda x: create_chat_prompt(sys_msg, x[\"Question\"], x['Paragraph'], x['Subject']), axis=1)\n",
    "        dataset[\"ideal\"] = dataset.Answer\n",
    "        samples_pth = os.path.join(subject_pth, \"samples.jsonl\")\n",
    "        dataset[[\"input\", \"ideal\"]].sample(frac=1).head(400).to_json(samples_pth, lines=True, orient=\"records\")\n",
    "        eval_id = f\"match_{data_pth}_train\"\n",
    "\n",
    "    registry_yaml[eval_id] = {\n",
    "        \"id\": f\"{eval_id}.test.v1\",\n",
    "        \"metrics\": [\"accuracy\"]\n",
    "    }\n",
    "    registry_yaml[f\"{eval_id}.test.v1\"] = {\n",
    "        \"class\": \"evals.elsuite.basic.match:Match\",\n",
    "        \"args\": {\n",
    "            \"samples_jsonl\": samples_pth,\n",
    "            \"few_shot_jsonl\": few_shot_pth,\n",
    "            \"num_few_shot\": 4,\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open(os.path.join(registry_pth, \"evals\", f\"{data_pth}.yaml\"), \"w\") as f:\n",
    "    yaml.dump(registry_yaml, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/Users/jackhopkins/miniforge3/bin/oaieval\", line 5, in <module>\r\n",
      "    from evals.cli.oaieval import main\r\n",
      "  File \"/Users/jackhopkins/PycharmProjects/evals/evals/__init__.py\", line 1, in <module>\r\n",
      "    from .api import check_sampled_text, completion_query, sample_freeform\r\n",
      "  File \"/Users/jackhopkins/PycharmProjects/evals/evals/api.py\", line 18, in <module>\r\n",
      "    from evals.utils.api_utils import (\r\n",
      "  File \"/Users/jackhopkins/PycharmProjects/evals/evals/utils/api_utils.py\", line 17, in <module>\r\n",
      "    openai.error.Timeout,\r\n",
      "AttributeError: module 'openai.error' has no attribute 'Timeout'\r\n"
     ]
    }
   ],
   "source": [
    "# This will generate a JSONL which will record samples and logs and store it in /tmp/evallogs\n",
    "!oaieval gpt-3.5-turbo match_squad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/evallogs/{log_name}'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [85]\u001B[0m, in \u001B[0;36m<cell line: 4>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# How to process the log events generated by oaieval\u001B[39;00m\n\u001B[1;32m      2\u001B[0m events \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/tmp/evallogs/\u001B[39m\u001B[38;5;132;01m{log_name}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mevents\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m      5\u001B[0m     events_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_json(f, lines\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      7\u001B[0m matches_df \u001B[38;5;241m=\u001B[39m events_df[events_df\u001B[38;5;241m.\u001B[39mtype \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmatch\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/tmp/evallogs/{log_name}'"
     ]
    }
   ],
   "source": [
    "# How to process the log events generated by oaieval\n",
    "events = \"/tmp/evallogs/{log_name}\"\n",
    "\n",
    "with open(events, \"r\") as f:\n",
    "    events_df = pd.read_json(f, lines=True)\n",
    "\n",
    "matches_df = events_df[events_df.type == \"match\"].reset_index(drop=True)\n",
    "matches_df = matches_df.join(pd.json_normalize(matches_df.data))\n",
    "matches_df.correct.value_counts().plot.bar(title=\"Correctness of generated answers\", xlabel=\"Correctness\", ylabel=\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect samples\n",
    "for i, r in pd.json_normalize(events_df[events_df.type == \"sampling\"].data).iterrows():\n",
    "    print(f\"Prompt: {r.prompt}\")\n",
    "    print(f\"Sampled: {r.sampled}\")\n",
    "    print(\"-\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}